import torch
from torch import autograd


up_sample_factor = 4


def calc_gradient_penalty(netD, real_data, fake_data, batch_size, l, device,
                          gp_lambda, nc):
    """
    calculate gradient penalty for a batch of real and fake data
    :param netD: Discriminator network
    :param real_data:
    :param fake_data:
    :param batch_size:
    :param l: image size
    :param device:
    :param gp_lambda: learning parameter for GP
    :param nc: channels
    :return: gradient penalty
    """
    #sample and reshape random numbers
    alpha = torch.rand(batch_size, 1, device = device)
    num_images = real_data.size()[0]
    alpha = alpha.expand(batch_size, int(real_data.numel() /
                                         batch_size)).contiguous()
    # print(alpha.shape)
    alpha = alpha.view(num_images, nc, l, l)

    # create interpolate dataset
    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())
    interpolates.requires_grad_(True)

    #pass interpolates through netD
    disc_interpolates = netD(interpolates)
    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,
                              grad_outputs=torch.ones(disc_interpolates.size(), device = device),
                              create_graph=True, only_inputs=True)[0]
    # extract the grads and calculate gp
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gp_lambda
    return gradient_penalty


def down_sample_for_g_input(high_res_3_phase, grey_idx, device):
    """
    :return: a down-sample of the grey material.
    """
    # first choose the grey phase in the image:
    grey_material = torch.index_select(high_res_3_phase, 1, grey_idx)
    # down sample:
    res = torch.nn.AvgPool2d(2, 2)(grey_material)
    res = torch.nn.AvgPool2d(2, 2)(res)
    # threshold at 0.5:
    res = torch.where(res > 0.5, 1., 0.)
    zeros_channel = torch.ones(size=res.size()).to(device) - res
    return torch.cat((zeros_channel, res), dim=1)


def up_sample_for_similarity_check(low_res_im, grey_idx):
    """
    Up-sample the low resolution image G gets as an input for pixel-wise
    similarity with the image generated by G.
    :param low_res_im: G input
    :param grey_idx: index for the grey material
    :return: An up sample (bilinearX4) of the low_res_im
    """
    grey_material = torch.index_select(low_res_im, 1, grey_idx)
    up_sample = torch.nn.Upsample(scale_factor=up_sample_factor,
                                  mode='bilinear')
    return up_sample(grey_material)


def pixel_wise_distance(low_res_im, generated_high_res, grey_idx):
    """
    calculates and returns the pixel wise distance between the low resolution
    image and the down sampling of the high resolution generated image.
    :return: the normalized distance (divided by the number of pixels of the
    low resolution image
    """
    generated_grey = torch.index_select(generated_high_res, 1, grey_idx)
    up_sample = up_sample_for_similarity_check(low_res_im, grey_idx)
    # h_r_n_pixels = torch.numel(up_sample[0, 0, :, :])
    # distance is the l2 norm calculating for each image in the batch:
    # dist = torch.sum((generated_grey-up_sample)**2, dim=[2, 3])/h_r_n_pixels
    # multiplying each image in the batch with the appropriate random number:
    # res = dist * low_res_im[:, 2, 0, 0]
    # print(dist.mean().item())
    # return the mean:
    return torch.nn.MSELoss()(generated_grey, up_sample)

